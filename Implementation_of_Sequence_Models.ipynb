{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybj0bWu1C9NB"
      },
      "source": [
        "# **PART A: Implementation of Sequence Models on the IMDB Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlKFKsceEqcT"
      },
      "source": [
        "**Step 1**: Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9zTp1XAEdaH",
        "outputId": "23c6eba1-927b-4af8-b0f6-4023044a24b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "review       0\n",
            "sentiment    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Import the Tokenizer and pad_sequences modules from tensorflow.keras.preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/DL LAB/IMDB Dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Check the first few rows of the dataset\n",
        "df.head()\n",
        "\n",
        "# Check for null values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Encode sentiment labels to binary (positive: 1, negative: 0)\n",
        "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X = df['review'].values\n",
        "y = df['sentiment'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "max_words = 5000\n",
        "max_len = 500\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4ustArFEwWn"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this code, we load the IMDB dataset from the provided CSV file and preprocess the data. We encode the target variable (sentiment) into binary format (1 for positive and 0 for negative). The dataset is split into training and testing sets, and the text reviews are tokenized and converted into sequences of integers. Finally, we pad the sequences to ensure they have the same length of 500 tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqTz8dAEE9xd"
      },
      "source": [
        "**Step 2:** Simple LSTM for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary Keras libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define a fraction of the dataset to use\n",
        "fraction = 0.1  # Use only 10% of the dataset\n",
        "\n",
        "# Sample a smaller portion of the training and test sets\n",
        "train_size = int(len(X_train_pad) * fraction)\n",
        "test_size = int(len(X_test_pad) * fraction)\n",
        "\n",
        "X_train_sample = X_train_pad[:train_size]\n",
        "y_train_sample = y_train[:train_size]\n",
        "X_test_sample = X_test_pad[:test_size]\n",
        "y_test_sample = y_test[:test_size]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 32, input_length=max_len))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the smaller dataset\n",
        "model.fit(X_train_sample, y_train_sample, epochs=3, batch_size=64, validation_data=(X_test_sample, y_test_sample))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_sample, y_test_sample, verbose=0)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEGtsGgfOnLq",
        "outputId": "7dec1807-b35b-43fb-d3e0-b5c369e1963a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 722ms/step - accuracy: 0.5317 - loss: 0.6925 - val_accuracy: 0.6100 - val_loss: 0.6559\n",
            "Epoch 2/3\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 713ms/step - accuracy: 0.7659 - loss: 0.5361 - val_accuracy: 0.7870 - val_loss: 0.4706\n",
            "Epoch 3/3\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 690ms/step - accuracy: 0.8754 - loss: 0.3361 - val_accuracy: 0.8070 - val_loss: 0.4412\n",
            "Accuracy: 80.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpAjjQlQL-c7"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this section, we build a simple LSTM model for sentiment analysis. The model consists of an embedding layer (which converts text into dense vectors), followed by an LSTM layer with 100 units. The final output layer is a dense layer with a sigmoid activation function to predict binary sentiment (positive or negative). The model is compiled with binary cross-entropy loss and Adam optimizer, and we train it for 3 epochs. The accuracy of the model on the test data is then evaluated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJZyROVcMJxI"
      },
      "source": [
        "**Step 3:** Fine-tuned Custom LSTM Model with Additional Layers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import additional layers\n",
        "from keras.layers import Dropout\n",
        "import numpy as np\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define a fraction of the dataset to use\n",
        "fraction = 0.1  # Use only 10% of the dataset\n",
        "\n",
        "# Sample a smaller portion of the training and test sets\n",
        "train_size = int(len(X_train_pad) * fraction)\n",
        "test_size = int(len(X_test_pad) * fraction)\n",
        "\n",
        "X_train_sample = X_train_pad[:train_size]\n",
        "y_train_sample = y_train[:train_size]\n",
        "X_test_sample = X_test_pad[:test_size]\n",
        "y_test_sample = y_test[:test_size]\n",
        "\n",
        "# Build the custom LSTM model with dropout and additional LSTM layers\n",
        "model_custom = Sequential()\n",
        "model_custom.add(Embedding(max_words, 64, input_length=max_len))\n",
        "model_custom.add(LSTM(128, return_sequences=True))\n",
        "model_custom.add(Dropout(0.3))\n",
        "model_custom.add(LSTM(64))\n",
        "model_custom.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_custom.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the custom model on the smaller dataset\n",
        "model_custom.fit(X_train_sample, y_train_sample, epochs=5, batch_size=64, validation_data=(X_test_sample, y_test_sample))\n",
        "\n",
        "# Evaluate the custom model\n",
        "loss_custom, accuracy_custom = model_custom.evaluate(X_test_sample, y_test_sample, verbose=0)\n",
        "print(f\"Custom Model Accuracy: {accuracy_custom * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAk71lrGQ0QE",
        "outputId": "0c6ff88e-a76c-4cbb-aa6b-eb57a1580b02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 2s/step - accuracy: 0.5525 - loss: 0.6808 - val_accuracy: 0.6990 - val_loss: 0.5936\n",
            "Epoch 2/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 2s/step - accuracy: 0.8121 - loss: 0.4250 - val_accuracy: 0.7660 - val_loss: 0.4986\n",
            "Epoch 3/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - accuracy: 0.8968 - loss: 0.2590 - val_accuracy: 0.7910 - val_loss: 0.4697\n",
            "Epoch 4/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 2s/step - accuracy: 0.9473 - loss: 0.1684 - val_accuracy: 0.7930 - val_loss: 0.5197\n",
            "Epoch 5/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - accuracy: 0.9602 - loss: 0.1205 - val_accuracy: 0.7820 - val_loss: 0.6268\n",
            "Custom Model Accuracy: 78.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I24ZghJpMVas"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here, we enhance the LSTM architecture by adding an additional LSTM layer with 128 units and a Dropout layer to prevent overfitting. The embedding size is increased to 64, and a dropout of 0.3 is applied between layers to improve generalization. The model is trained for 5 epochs and evaluated for performance on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFN1i9NFMazz"
      },
      "source": [
        "**Step 4:** Bidirectional LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Bidirectional wrapper\n",
        "from keras.layers import Bidirectional\n",
        "import numpy as np\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define a fraction of the dataset to use\n",
        "fraction = 0.1  # Use only 10% of the dataset\n",
        "\n",
        "# Sample a smaller portion of the training and test sets\n",
        "train_size = int(len(X_train_pad) * fraction)\n",
        "test_size = int(len(X_test_pad) * fraction)\n",
        "\n",
        "X_train_sample = X_train_pad[:train_size]\n",
        "y_train_sample = y_train[:train_size]\n",
        "X_test_sample = X_test_pad[:test_size]\n",
        "y_test_sample = y_test[:test_size]\n",
        "\n",
        "# Build a Bidirectional LSTM model\n",
        "model_blstm = Sequential()\n",
        "model_blstm.add(Embedding(max_words, 64, input_length=max_len))\n",
        "model_blstm.add(Bidirectional(LSTM(100)))\n",
        "model_blstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_blstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the Bidirectional LSTM model on the smaller dataset\n",
        "model_blstm.fit(X_train_sample, y_train_sample, epochs=5, batch_size=64, validation_data=(X_test_sample, y_test_sample))\n",
        "\n",
        "# Evaluate the Bidirectional LSTM model\n",
        "loss_blstm, accuracy_blstm = model_blstm.evaluate(X_test_sample, y_test_sample, verbose=0)\n",
        "print(f\"Bidirectional LSTM Model Accuracy: {accuracy_blstm * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2_fLlQPRPS7",
        "outputId": "5774f539-85a1-4cd3-e024-4ae1408961cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 2s/step - accuracy: 0.5466 - loss: 0.6781 - val_accuracy: 0.7120 - val_loss: 0.6152\n",
            "Epoch 2/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.7567 - loss: 0.5677 - val_accuracy: 0.8110 - val_loss: 0.4267\n",
            "Epoch 3/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - accuracy: 0.8713 - loss: 0.3219 - val_accuracy: 0.8050 - val_loss: 0.4248\n",
            "Epoch 4/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - accuracy: 0.9334 - loss: 0.2049 - val_accuracy: 0.8010 - val_loss: 0.5207\n",
            "Epoch 5/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - accuracy: 0.9563 - loss: 0.1404 - val_accuracy: 0.8200 - val_loss: 0.4672\n",
            "Bidirectional LSTM Model Accuracy: 82.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvlHh4ieMtsp"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "---\n",
        "\n",
        "This section demonstrates the implementation of a Bidirectional LSTM model. Bidirectional layers allow the model to process the sequence in both forward and backward directions, which can improve the model’s ability to capture the context of words in a sentence. The rest of the model structure remains similar to the previous ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fW_uSTyMzqM"
      },
      "source": [
        "**Step 5:** GRU and Bidirectional GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import GRU layer\n",
        "from keras.layers import GRU\n",
        "import numpy as np\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define a fraction of the dataset to use\n",
        "fraction = 0.1  # Use only 10% of the dataset\n",
        "\n",
        "# Sample a smaller portion of the training and test sets\n",
        "train_size = int(len(X_train_pad) * fraction)\n",
        "test_size = int(len(X_test_pad) * fraction)\n",
        "\n",
        "X_train_sample = X_train_pad[:train_size]\n",
        "y_train_sample = y_train[:train_size]\n",
        "X_test_sample = X_test_pad[:test_size]\n",
        "y_test_sample = y_test[:test_size]\n",
        "\n",
        "# Build a GRU model\n",
        "model_gru = Sequential()\n",
        "model_gru.add(Embedding(max_words, 64, input_length=max_len))\n",
        "model_gru.add(GRU(100))\n",
        "model_gru.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the GRU model on the smaller dataset\n",
        "model_gru.fit(X_train_sample, y_train_sample, epochs=5, batch_size=64, validation_data=(X_test_sample, y_test_sample))\n",
        "\n",
        "# Evaluate the GRU model\n",
        "loss_gru, accuracy_gru = model_gru.evaluate(X_test_sample, y_test_sample, verbose=0)\n",
        "print(f\"GRU Model Accuracy: {accuracy_gru * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWYc_8N8Shx6",
        "outputId": "e2d89c40-09e6-41b0-c408-6dfd3f01d1e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 940ms/step - accuracy: 0.5541 - loss: 0.6878 - val_accuracy: 0.6540 - val_loss: 0.6451\n",
            "Epoch 2/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 861ms/step - accuracy: 0.7287 - loss: 0.6005 - val_accuracy: 0.6980 - val_loss: 0.5731\n",
            "Epoch 3/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 824ms/step - accuracy: 0.8298 - loss: 0.4205 - val_accuracy: 0.7760 - val_loss: 0.4621\n",
            "Epoch 4/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 823ms/step - accuracy: 0.9250 - loss: 0.2060 - val_accuracy: 0.7770 - val_loss: 0.5140\n",
            "Epoch 5/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 828ms/step - accuracy: 0.9503 - loss: 0.1436 - val_accuracy: 0.8010 - val_loss: 0.5866\n",
            "GRU Model Accuracy: 80.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a Bidirectional GRU model\n",
        "from keras.layers import Bidirectional, GRU\n",
        "import numpy as np\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define a fraction of the dataset to use\n",
        "fraction = 0.1  # Use only 10% of the dataset\n",
        "\n",
        "# Sample a smaller portion of the training and test sets\n",
        "train_size = int(len(X_train_pad) * fraction)\n",
        "test_size = int(len(X_test_pad) * fraction)\n",
        "\n",
        "X_train_sample = X_train_pad[:train_size]\n",
        "y_train_sample = y_train[:train_size]\n",
        "X_test_sample = X_test_pad[:test_size]\n",
        "y_test_sample = y_test[:test_size]\n",
        "\n",
        "# Build a Bidirectional GRU model\n",
        "model_bgru = Sequential()\n",
        "model_bgru.add(Embedding(max_words, 64, input_length=max_len))\n",
        "model_bgru.add(Bidirectional(GRU(100)))\n",
        "model_bgru.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model_bgru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the Bidirectional GRU model on the smaller dataset\n",
        "model_bgru.fit(X_train_sample, y_train_sample, epochs=5, batch_size=64, validation_data=(X_test_sample, y_test_sample))\n",
        "\n",
        "# Evaluate the Bidirectional GRU model\n",
        "loss_bgru, accuracy_bgru = model_bgru.evaluate(X_test_sample, y_test_sample, verbose=0)\n",
        "print(f\"Bidirectional GRU Model Accuracy: {accuracy_bgru * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p75cnxiOSsGy",
        "outputId": "afc4d7ce-6e20-463d-fa9e-09cdab5d63cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 2s/step - accuracy: 0.5372 - loss: 0.6871 - val_accuracy: 0.6710 - val_loss: 0.5893\n",
            "Epoch 2/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 2s/step - accuracy: 0.8016 - loss: 0.4525 - val_accuracy: 0.7920 - val_loss: 0.4231\n",
            "Epoch 3/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 2s/step - accuracy: 0.9040 - loss: 0.2530 - val_accuracy: 0.7940 - val_loss: 0.4674\n",
            "Epoch 4/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 2s/step - accuracy: 0.9402 - loss: 0.1824 - val_accuracy: 0.7810 - val_loss: 0.5203\n",
            "Epoch 5/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.9469 - loss: 0.1490 - val_accuracy: 0.7890 - val_loss: 0.6176\n",
            "Bidirectional GRU Model Accuracy: 78.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-JFv2V1M4nd"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this final section, we implement both GRU and Bidirectional GRU models. GRU (Gated Recurrent Unit) is another type of RNN that is similar to LSTM but with fewer parameters, making it faster to train while achieving comparable results. Both unidirectional and bidirectional versions of GRU are implemented here and their accuracy on the test data is evaluated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKAklBf0N_cS"
      },
      "source": [
        "**Step 6**: Evaluation of Models (LSTM, BLSTM, GRU, BGRU) in Terms of Precision, Recall, and F1-Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xYTHhKOKODvg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0947737-3a70-47f1-b299-343d8ac83f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 101ms/step\n",
            "Performance metrics for LSTM:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.85      0.77      0.80      4961\n",
            "    Positive       0.79      0.86      0.82      5039\n",
            "\n",
            "    accuracy                           0.81     10000\n",
            "   macro avg       0.82      0.81      0.81     10000\n",
            "weighted avg       0.82      0.81      0.81     10000\n",
            "\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 187ms/step\n",
            "Performance metrics for Bidirectional LSTM:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.79      0.87      0.83      4961\n",
            "    Positive       0.86      0.77      0.81      5039\n",
            "\n",
            "    accuracy                           0.82     10000\n",
            "   macro avg       0.82      0.82      0.82     10000\n",
            "weighted avg       0.82      0.82      0.82     10000\n",
            "\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 102ms/step\n",
            "Performance metrics for GRU:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.79      0.84      0.81      4961\n",
            "    Positive       0.83      0.77      0.80      5039\n",
            "\n",
            "    accuracy                           0.81     10000\n",
            "   macro avg       0.81      0.81      0.81     10000\n",
            "weighted avg       0.81      0.81      0.81     10000\n",
            "\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 182ms/step\n",
            "Performance metrics for Bidirectional GRU:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.76      0.86      0.81      4961\n",
            "    Positive       0.85      0.74      0.79      5039\n",
            "\n",
            "    accuracy                           0.80     10000\n",
            "   macro avg       0.81      0.80      0.80     10000\n",
            "weighted avg       0.81      0.80      0.80     10000\n",
            "\n",
            "                    Precision    Recall  F1-score\n",
            "LSTM                 0.788891  0.862473  0.824042\n",
            "Bidirectional LSTM   0.857395  0.770788  0.811788\n",
            "GRU                  0.832870  0.773368  0.802017\n",
            "Bidirectional GRU    0.846364  0.736853  0.787821\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for performance metrics\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "\n",
        "# Function to evaluate models and print precision, recall, f1\n",
        "def evaluate_model(model, X_test_pad, y_test, model_name):\n",
        "    y_pred = model.predict(X_test_pad)\n",
        "    y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to class labels (0 or 1)\n",
        "\n",
        "    # Generate the classification report\n",
        "    report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'], output_dict=True)\n",
        "    print(f\"Performance metrics for {model_name}:\\n\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
        "\n",
        "    # Return the main metrics (precision, recall, f1) for Positive class\n",
        "    return report['Positive']['precision'], report['Positive']['recall'], report['Positive']['f1-score']\n",
        "\n",
        "# Evaluate all models and tabulate results\n",
        "models = {\n",
        "    \"LSTM\": model,\n",
        "    \"Bidirectional LSTM\": model_blstm,\n",
        "    \"GRU\": model_gru,\n",
        "    \"Bidirectional GRU\": model_bgru\n",
        "}\n",
        "\n",
        "performance_metrics = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    precision, recall, f1_score = evaluate_model(model, X_test_pad, y_test, name)\n",
        "    performance_metrics[name] = [precision, recall, f1_score]\n",
        "\n",
        "# Display results in a tabular format using pandas\n",
        "performance_df = pd.DataFrame(performance_metrics, index=['Precision', 'Recall', 'F1-score'])\n",
        "performance_df = performance_df.T  # Transpose to have models as rows\n",
        "print(performance_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hDvYJjdOLDb"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   The evaluate_model function is used to predict the sentiment using the trained model and compute the Precision, Recall, and F1-Score for each model.\n",
        "*   The predictions are made on the test data (X_test_pad), and we convert probabilities to class labels (0 for negative and 1 for positive).\n",
        "\n",
        "*  The classification_report function from sklearn provides a detailed breakdown of performance metrics for both classes (Negative and Positive). We focus on the metrics for the Positive class (since it's a binary classification problem).\n",
        "*   We loop through all models (LSTM, BLSTM, GRU, BGRU) to generate the metrics and store them in a dictionary. Finally, we tabulate the results using pandas.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}